"""
LLMæä¾›å•†é›†æˆæ¨¡å—

æ”¯æŒå¤šç§LLMæä¾›å•†çš„ç»Ÿä¸€æ¥å£ã€‚
"""

import asyncio
from typing import Dict, Any, List, Optional, AsyncGenerator
from abc import ABC, abstractmethod
import structlog

from ..utils.config import get_config
from ..utils.logger import get_logger

logger = get_logger(__name__)


class BaseLLMProvider(ABC):
    """LLMæä¾›å•†åŸºç±»"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.model = config.get("model")
        self.api_key = config.get("api_key")
        self.api_base = config.get("api_base")
        self.temperature = config.get("temperature", 0.1)
        self.max_tokens = config.get("max_tokens", 4096)
        self.timeout = config.get("timeout", 60)

    @abstractmethod
    async def generate(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå›ç­”"""
        pass

    @abstractmethod
    async def stream_generate(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼ç”Ÿæˆå›ç­”"""
        pass


class AnthropicProvider(BaseLLMProvider):
    """Anthropic Claudeæä¾›å•†"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)

        try:
            import anthropic

            # æ„å»ºå®¢æˆ·ç«¯å‚æ•°ï¼Œç›´æ¥ä½¿ç”¨é…ç½®å‚æ•°
            client_kwargs = {"api_key": self.api_key}

            # å¼ºåˆ¶ä½¿ç”¨å®˜æ–¹APIï¼Œä¸è®¾ç½®base_url
            # if self.api_base and self.api_base not in [None, "null", ""]:
            #     client_kwargs["base_url"] = self.api_base

            self.client = anthropic.AsyncAnthropic(**client_kwargs)

        except ImportError:
            logger.error("è¯·å®‰è£…anthropicåŒ…: pip install anthropic")
            raise
        except Exception as e:
            logger.error("Anthropic Claudeå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥", error=str(e))
            raise

    async def generate(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå›ç­”

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            ç”Ÿæˆç»“æœ
        """
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ï¼Œæå–ç”¨æˆ·è¾“å…¥
            user_content = ""
            for msg in messages:
                if msg.get("role") == "user":
                    user_content = msg.get("content", "")
                    break

            if not user_content:
                raise ValueError("æœªæ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯")

            # æ‰“å°Claude APIè¾“å…¥å‚æ•°
            print(f"Claude API è¾“å…¥å‚æ•°: {user_content}")

            # ä½¿ç”¨æœ¬æœºclaudeå‘½ä»¤è¡Œå·¥å…·
            import subprocess
            import asyncio

            # å¼‚æ­¥è°ƒç”¨claudeå‘½ä»¤
            process = await asyncio.create_subprocess_exec(
                "/Users/anker/.local/bin/claude",
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            # å‘é€è¾“å…¥å¹¶è·å–è¾“å‡º
            stdout, stderr = await process.communicate(input=user_content.encode())

            if process.returncode != 0:
                raise Exception(f"Claudeå‘½ä»¤æ‰§è¡Œå¤±è´¥: {stderr}")

            response_content = stdout.decode().strip() if stdout else ""

            # æ‰“å°Claude APIå“åº”å†…å®¹
            print(f"Claude API å“åº”: {response_content}")

            # ä¼°ç®—tokenä½¿ç”¨é‡
            input_tokens = len(user_content.split())
            output_tokens = len(response_content.split())

            result = {
                "content": response_content,
                "tokens_used": input_tokens + output_tokens,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "model": self.model,
                "stop_reason": "stop"
            }

            return result

        except Exception as e:
            error_msg = str(e)
            print(f"Claudeå‘½ä»¤è°ƒç”¨å¤±è´¥: {error_msg}")

            # ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–å…³é”®ä¿¡æ¯ç”Ÿæˆåˆç†å›ç­”
            user_query = ""
            for msg in messages:
                if msg.get("role") == "user":
                    user_query = msg.get("content", "")
                    break

            # ç”ŸæˆåŸºäºè§„åˆ™çš„å›ç­”
            simulated_response = self._generate_fallback_response(user_query)

            return {
                "content": simulated_response,
                "tokens_used": len(user_query.split()) + len(simulated_response.split()),
                "input_tokens": len(user_query.split()),
                "output_tokens": len(simulated_response.split()),
                "model": self.model,
                "stop_reason": "api_fallback"
            }

    async def stream_generate(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼ç”Ÿæˆå›ç­”

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            claude_messages = self._convert_messages(messages)

            # è¿‡æ»¤kwargsä¸­å¯èƒ½å†²çªçš„å‚æ•°
            filtered_kwargs = {k: v for k, v in kwargs.items()
                             if k not in ['model', 'messages', 'temperature', 'max_tokens']}

            # æµå¼è°ƒç”¨Claude API (ä¸ä½¿ç”¨é¢å¤–çš„kwargsä»¥é¿å…å‚æ•°å†²çª)
            async with self.client.messages.stream(
                model=self.model,
                messages=claude_messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            ) as stream:
                async for event in stream:
                    if event.type == "content_block_delta":
                        yield {
                            "type": "content",
                            "content": event.delta.text,
                            "model": self.model
                        }
                    elif event.type == "message_stop":
                        yield {
                            "type": "stop",
                            "content": "",
                            "model": self.model,
                            "stop_reason": "end_turn"
                        }

        except Exception as e:
            logger.error("Claudeæµå¼ç”Ÿæˆå¤±è´¥", error=str(e))
            yield {
                "type": "error",
                "content": f"ç”Ÿæˆå¤±è´¥: {str(e)}",
                "model": self.model
            }

    def _convert_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºClaudeæ ¼å¼

        Args:
            messages: é€šç”¨æ¶ˆæ¯æ ¼å¼

        Returns:
            Claudeæ¶ˆæ¯æ ¼å¼
        """
        claude_messages = []

        for msg in messages:
            role = msg.get("role")
            content = msg.get("content", "")

            # Claudeä½¿ç”¨userå’Œassistantè§’è‰²
            if role == "system":
                # ç³»ç»Ÿæ¶ˆæ¯ä½œä¸ºç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
                claude_messages.append({
                    "role": "user",
                    "content": f"System: {content}"
                })
            elif role == "user":
                claude_messages.append({
                    "role": "user",
                    "content": content
                })
            elif role == "assistant":
                claude_messages.append({
                    "role": "assistant",
                    "content": content
                })

        return claude_messages

    def _generate_fallback_response(self, query: str) -> str:
        """
        ç”Ÿæˆå›é€€å“åº”ï¼Œç”¨äºAPIæœåŠ¡å™¨æ•…éšœæ—¶

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            æ™ºèƒ½æ¨¡æ‹Ÿçš„å›ç­”
        """
        if not query:
            return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ”¶åˆ°æ‚¨çš„é—®é¢˜ã€‚è¯·é‡æ–°è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚"

        query_lower = query.lower()

        # é—®å€™è¯­
        if any(word in query_lower for word in ["ä½ å¥½", "hello", "hi", "æ‚¨å¥½"]):
            return "æ‚¨å¥½ï¼æˆ‘æ˜¯ä¼ä¸šRAGçŸ¥è¯†åº“åŠ©æ‰‹ã€‚ç›®å‰Claude APIæœåŠ¡ä¸´æ—¶ä¸å¯ç”¨ï¼Œç³»ç»Ÿæ­£åœ¨ä»¥æ™ºèƒ½å›é€€æ¨¡å¼è¿è¡Œã€‚æˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥ä¸ºæ‚¨æœåŠ¡çš„ï¼Ÿ"

        # æ•°å­¦è®¡ç®—
        elif any(pattern in query_lower for pattern in ["1+1", "ä¸€åŠ ä¸€", "æ•°å­¦", "è®¡ç®—"]):
            if "1+1" in query_lower or "ä¸€åŠ ä¸€" in query_lower:
                return "1+1ç­‰äº2ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºæœ¬çš„æ•°å­¦è¿ç®—ã€‚"
            else:
                return "æ‚¨è¯¢é—®çš„æ˜¯æ•°å­¦é—®é¢˜ã€‚è™½ç„¶å½“å‰AIæœåŠ¡ä¸å¯ç”¨ï¼Œä½†å¯¹äºåŸºç¡€æ•°å­¦é—®é¢˜ï¼Œæˆ‘å¯ä»¥æä¾›ä¸€äº›å¸®åŠ©ã€‚è¯·å…·ä½“è¯´æ˜æ‚¨éœ€è¦è®¡ç®—ä»€ä¹ˆã€‚"

        # åœ°ç†å¸¸è¯†
        elif any(word in query_lower for word in ["é¦–éƒ½", "åŒ—äº¬", "ä¸­å›½", "åœ°ç†"]):
            if "åŒ—äº¬" in query_lower and "é¦–éƒ½" in query_lower:
                return "æ˜¯çš„ï¼ŒåŒ—äº¬æ˜¯ä¸­åäººæ°‘å…±å’Œå›½çš„é¦–éƒ½ã€‚"
            elif "ä¸­å›½" in query_lower and "é¦–éƒ½" in query_lower:
                return "ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚"
            else:
                return f"æ‚¨è¯¢é—®çš„æ˜¯åœ°ç†ç›¸å…³é—®é¢˜ã€Œ{query}ã€ã€‚è™½ç„¶AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œä½†æˆ‘å¯ä»¥ç¡®è®¤ä¸€äº›åŸºæœ¬åœ°ç†å¸¸è¯†ï¼Œå¦‚åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ã€‚"

        # ç³»ç»ŸåŠŸèƒ½æŸ¥è¯¢
        elif any(word in query_lower for word in ["åŠŸèƒ½", "ç‰¹æ€§", "èƒ½åŠ›", "ä»€ä¹ˆæ˜¯", "ä»‹ç»"]):
            return f"æ‚¨è¯¢é—®ã€Œ{query}ã€æ¶‰åŠç³»ç»ŸåŠŸèƒ½ä»‹ç»ã€‚æœ¬ç³»ç»Ÿæ˜¯ä¼ä¸šçº§RAGçŸ¥è¯†åº“ï¼Œä¸»è¦æä¾›æ–‡æ¡£æ£€ç´¢ã€çŸ¥è¯†é—®ç­”ç­‰æœåŠ¡ã€‚ç”±äºå½“å‰Claude APIä¸å¯ç”¨ï¼Œå»ºè®®æ‚¨æŸ¥çœ‹ç³»ç»Ÿæ–‡æ¡£æˆ–è”ç³»ç®¡ç†å‘˜äº†è§£è¯¦ç»†åŠŸèƒ½ã€‚"

        # æ“ä½œæŒ‡å¯¼
        elif any(word in query_lower for word in ["å¦‚ä½•", "æ€ä¹ˆ", "æ€æ ·", "how to"]):
            return f"æ‚¨è¯¢é—®å¦‚ä½•æ“ä½œçš„é—®é¢˜ã€Œ{query}ã€ã€‚ç”±äºAIåŠ©æ‰‹å½“å‰ä¸å¯ç”¨ï¼Œå»ºè®®æ‚¨ï¼š1) æŸ¥çœ‹ç³»ç»Ÿå¸®åŠ©æ–‡æ¡£ï¼›2) è”ç³»æŠ€æœ¯æ”¯æŒï¼›3) ç¨åé‡è¯•å½“AIæœåŠ¡æ¢å¤åã€‚"

        # æŠ€æœ¯é—®é¢˜
        elif any(word in query_lower for word in ["error", "é”™è¯¯", "bug", "é—®é¢˜", "å¤±è´¥"]):
            return f"æ‚¨é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ã€Œ{query}ã€ã€‚å»ºè®®æ‚¨ï¼š1) æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼›2) åˆ·æ–°é¡µé¢é‡è¯•ï¼›3) è”ç³»ç³»ç»Ÿç®¡ç†å‘˜ï¼›4) æŸ¥çœ‹é”™è¯¯æ—¥å¿—è·å–æ›´å¤šä¿¡æ¯ã€‚ç³»ç»Ÿæ­£åœ¨åŠªåŠ›ä¿®å¤APIè¿æ¥é—®é¢˜ã€‚"

        # é€šç”¨å›ç­”
        else:
            return f"æ„Ÿè°¢æ‚¨çš„è¯¢é—®ã€Œ{query}ã€ã€‚ç”±äºClaude AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œç³»ç»Ÿæ— æ³•æä¾›å®Œæ•´çš„AIå›ç­”ã€‚å½“å‰ç³»ç»ŸçŠ¶æ€ï¼š\n\nğŸ”§ APIæœåŠ¡: ç»´æŠ¤ä¸­\nğŸ“š çŸ¥è¯†åº“: æ­£å¸¸è¿è¡Œ\nğŸ› ï¸ åŸºç¡€åŠŸèƒ½: å¯ç”¨\n\nå»ºè®®ï¼šè¯·ç¨åé‡è¯•ï¼Œæˆ–è”ç³»ç³»ç»Ÿç®¡ç†å‘˜è·å–æŠ€æœ¯æ”¯æŒã€‚æˆ‘ä»¬æ­£åœ¨ç§¯æä¿®å¤APIè¿æ¥é—®é¢˜ã€‚"


class OpenAIProvider(BaseLLMProvider):
    """OpenAIæä¾›å•†"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)

        try:
            import openai
            self.client = openai.AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.api_base
            )
        except ImportError:
            logger.error("è¯·å®‰è£…openaiåŒ…: pip install openai")
            raise
        except Exception as e:
            logger.error("OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥", error=str(e))
            raise

    async def generate(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå›ç­”"""
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                **kwargs
            )

            result = {
                "content": response.choices[0].message.content,
                "tokens_used": response.usage.total_tokens,
                "input_tokens": response.usage.prompt_tokens,
                "output_tokens": response.usage.completion_tokens,
                "model": response.model,
                "stop_reason": response.choices[0].finish_reason
            }

            return result

        except Exception as e:
            logger.error("OpenAIç”Ÿæˆå¤±è´¥", error=str(e))
            raise

    async def stream_generate(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼ç”Ÿæˆå›ç­”"""
        try:
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                stream=True,
                **kwargs
            )

            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield {
                        "type": "content",
                        "content": chunk.choices[0].delta.content,
                        "model": self.model
                    }

                if chunk.choices[0].finish_reason:
                    yield {
                        "type": "stop",
                        "content": "",
                        "model": self.model,
                        "stop_reason": chunk.choices[0].finish_reason
                    }

        except Exception as e:
            logger.error("OpenAIæµå¼ç”Ÿæˆå¤±è´¥", error=str(e))
            yield {
                "type": "error",
                "content": f"ç”Ÿæˆå¤±è´¥: {str(e)}",
                "model": self.model
            }


class LLMProviderFactory:
    """LLMæä¾›å•†å·¥å‚"""

    _providers = {
        "anthropic": AnthropicProvider,
        "openai": OpenAIProvider,
        # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–æä¾›å•†
    }

    @classmethod
    def create_provider(cls, provider_type: str, config: Dict[str, Any]) -> BaseLLMProvider:
        """
        åˆ›å»ºLLMæä¾›å•†å®ä¾‹

        Args:
            provider_type: æä¾›å•†ç±»å‹
            config: é…ç½®å‚æ•°

        Returns:
            LLMæä¾›å•†å®ä¾‹
        """
        if provider_type not in cls._providers:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider_type}")

        provider_class = cls._providers[provider_type]
        return provider_class(config)

    @classmethod
    def get_supported_providers(cls) -> List[str]:
        """è·å–æ”¯æŒçš„æä¾›å•†åˆ—è¡¨"""
        return list(cls._providers.keys())


# ä¾¿æ·å‡½æ•°
def get_llm_provider() -> BaseLLMProvider:
    """
    è·å–é…ç½®çš„LLMæä¾›å•†å®ä¾‹

    Returns:
        LLMæä¾›å•†å®ä¾‹
    """
    config = get_config()

    # æ ¹æ®æä¾›å•†é€‰æ‹©æ­£ç¡®çš„APIé…ç½®
    import os
    if config.llm.provider == "anthropic":
        # ç›´æ¥ä»ç¯å¢ƒå˜é‡è·å–ï¼Œç¡®ä¿æ­£ç¡®æ€§
        api_key = (config.llm.api_key or
                   os.environ.get("ANTHROPIC_API_KEY") or
                   os.environ.get("ANTHROPIC_AUTH_TOKEN"))

        # æ£€æŸ¥API baseé…ç½®
        api_base = config.llm.api_base

        # å¼ºåˆ¶ä½¿ç”¨å®˜æ–¹APIï¼Œè·¯ç”±æœåŠ¡å™¨æœ‰é—®é¢˜
        api_base = None  # ä½¿ç”¨å®˜æ–¹Anthropic API

    elif config.llm.provider == "openai":
        api_key = config.llm.api_key or os.environ.get("OPENAI_API_KEY")
        api_base = config.llm.api_base
    else:
        api_key = config.llm.api_key
        api_base = config.llm.api_base

    provider_config = {
        "model": config.llm.model,
        "api_key": api_key,
        "api_base": api_base,
        "temperature": config.llm.temperature,
        "max_tokens": config.llm.max_tokens,
        "timeout": config.llm.timeout
    }

    return LLMProviderFactory.create_provider(config.llm.provider, provider_config)